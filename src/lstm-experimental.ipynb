{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook contains experimental training of LSTM with pytorch lightning\r\n",
    "\r\n",
    "There is still a bug when shuffling data for training (see dataloader) but it seems to work better without shuffling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pytorch_lightning\r\n",
    "from pytorch_lightning import Trainer, LightningModule\r\n",
    "import torch\r\n",
    "from data_classes.IMDB import IMDBClass\r\n",
    "import torchtext"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prepare pretrained glove embeddings\r\n",
    "glove_vec = torchtext.vocab.GloVe(max_vectors=10000)\r\n",
    "glove_vocab = torchtext.vocab.vocab(glove_vec.stoi)\r\n",
    "unk_token = \"<unk>\"\r\n",
    "unk_index = 0\r\n",
    "glove_vocab.insert_token(unk_token, unk_index)\r\n",
    "glove_vocab.set_default_index(glove_vocab[unk_token])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prepare datasets\r\n",
    "train_dataset = IMDBClass(train=True, transform=glove_vocab)\r\n",
    "test_dataset = IMDBClass(train=False, transform=glove_vocab)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prepare DataLoader\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "def collate_fn(batch):\r\n",
    "    x = [item[0] for item in batch]\r\n",
    "    lengths = torch.LongTensor(list(map(len, x)))\r\n",
    "    x = pad_sequence(x, batch_first=True)\r\n",
    "    y = torch.tensor([item[1] for item in batch], dtype=torch.long)\r\n",
    "    return x, y, lengths\r\n",
    "\r\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn)\r\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model and system definition\r\n",
    "from torch.optim import Adam\r\n",
    "from torch import nn\r\n",
    "from torch.nn.utils.rnn import pad_sequence\r\n",
    "\r\n",
    "class LSTM(LightningModule):\r\n",
    "    def __init__(self, vocab_size=None, embedding_size=64, lstm_hidden_size=100, num_class=2, batch_size=32, learning_rate=0.001, vocab=None, vectors=None):\r\n",
    "        super().__init__()\r\n",
    "        if vocab is None:\r\n",
    "            self.embedding = torch.nn.Embedding(vocab_size, embedding_size, padding_idx=0)\r\n",
    "        else:\r\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(vectors.vectors, freeze=True, padding_idx=vocab[\"<pad>\"])\r\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\r\n",
    "        self.linear = nn.Linear(lstm_hidden_size, num_class)\r\n",
    "        self.loss_function = nn.CrossEntropyLoss()\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.lstm_hidden_size = lstm_hidden_size\r\n",
    "    \r\n",
    "    def forward(self, X: torch.Tensor, lengths: torch.LongTensor):\r\n",
    "        x = self.embedding(X)\r\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths.to(\"cpu\"), enforce_sorted=False, batch_first=True)\r\n",
    "        _, (hn, _) = self.lstm(x)\r\n",
    "        hn = hn[-1,:,:]\r\n",
    "        x = self.linear(hn)\r\n",
    "        return x\r\n",
    "    \r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        x, y, lengths = batch\r\n",
    "        y_hat = self(x, lengths)\r\n",
    "        loss = self.loss_function(y_hat, y)\r\n",
    "        self.log(\"Train Loss\", loss.detach())\r\n",
    "        return loss\r\n",
    "           \r\n",
    "    \r\n",
    "    def configure_optimizers(self):\r\n",
    "        return Adam(self.parameters(), lr=1e-2)\r\n",
    "        \r\n",
    "    def test_step(self, batch, batch_idx):\r\n",
    "        x, y, lengths = batch\r\n",
    "        y_hat = self(x, lengths)\r\n",
    "        loss = self.loss_function(y_hat, y)\r\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\r\n",
    "        test_acc = torch.sum(labels_hat == y).item() / (len(y) * 1.0)\r\n",
    "        return self.log_dict({'Test Loss': loss, 'Test Acc': test_acc})\r\n",
    "    \r\n",
    "    def train_dataloader(self):\r\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn) # collate_fn=collate_fn\r\n",
    "\r\n",
    "    \r\n",
    "    def test_dataloader(self):\r\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Instantiate model\r\n",
    "model = LSTM(embedding_size=300, num_class=2, vocab=glove_vocab, vectors=glove_vec)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Instantiate Logger\r\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\r\n",
    "logger = TensorBoardLogger('exp_logs', name='lstm')\r\n",
    "# run tensorboard with:\r\n",
    "# tensorboard --logdir exp_logs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Instantiate trainer\r\n",
    "trainer = Trainer(max_epochs=10, gpus=1, auto_select_gpus=True, auto_scale_batch_size=False, auto_lr_find=True, logger=[logger], track_grad_norm=2, \r\n",
    "accumulate_grad_batches=8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.tune(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.learning_rate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "trainer.test(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.fit(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.test(model, test_dataloader)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('deep_learning': conda)"
  },
  "interpreter": {
   "hash": "cfa97515b48ec924051b6584906c775d898061829fcbf4ab570c29d743c4fef7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}