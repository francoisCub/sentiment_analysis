{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook contains experimental training of LSTM with pytorch lightning\r\n",
    "\r\n",
    "There is still a bug when shuffling data for training (see dataloader) but it seems to work better without shuffling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pytorch_lightning\r\n",
    "from pytorch_lightning import Trainer, LightningModule\r\n",
    "import torch\r\n",
    "from data_classes.IMDB import IMDBClass\r\n",
    "import torchtext"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_vec = torchtext.vocab.GloVe(max_vectors=10000)\r\n",
    "glove_vocab = torchtext.vocab.vocab(glove_vec.stoi)\r\n",
    "unk_token = \"<unk>\"\r\n",
    "unk_index = 0\r\n",
    "# pad_token = \"<pad>\"\r\n",
    "# pad_index = 9999\r\n",
    "glove_vocab.insert_token(unk_token, unk_index)\r\n",
    "# glove_vocab.insert_token(pad_token, pad_index)\r\n",
    "\r\n",
    "# #this is necessary otherwise it will throw runtime error if OOV token is queried \r\n",
    "glove_vocab.set_default_index(glove_vocab[unk_token])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_vocab[\"kejslskgjfd\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(glove_vec)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(glove_vocab)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_vocab.lookup_token(9999)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_vec[\"<unk>\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_vocab[\"<pad>\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = IMDBClass(train=True, transform=glove_vocab)\r\n",
    "test_dataset = IMDBClass(train=False, transform=glove_vocab)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\r\n",
    "# For use in DataLoader\r\n",
    "def collate_fn(batch):\r\n",
    "    x = [item[0] for item in batch]\r\n",
    "\r\n",
    "    # maxes = []\r\n",
    "    # for it in x:\r\n",
    "    #     maxes.append(it.max())\r\n",
    "    # print(torch.tensor(maxes).max())\r\n",
    "\r\n",
    "    lengths = torch.LongTensor(list(map(len, x)))\r\n",
    "    x = pad_sequence(x, batch_first=True)\r\n",
    "    y = torch.tensor([item[1] for item in batch], dtype=torch.long)\r\n",
    "    return x, y, lengths\r\n",
    "\r\n",
    "# Warning: there is a bug somewhere in the code, when shuffle=True the model doesn't learn anymore\r\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn) # collate_fn=collate_fn\r\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn) # collate_fn=collate_fn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(glove_vocab)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glove_vocab.lookup_indices([\"the\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.optim import Adam\r\n",
    "from torch import nn\r\n",
    "from torch.nn.utils.rnn import pad_sequence\r\n",
    "\r\n",
    "nn.Module\r\n",
    "class LSTM(LightningModule):\r\n",
    "    def __init__(self, vocab_size, embedding_size=64, lstm_hidden_size=100, num_class=2, batch_size=32, learning_rate=0.001, vocab=None, vectors=None):\r\n",
    "        super().__init__()\r\n",
    "        if vocab is None:\r\n",
    "            self.embedding = torch.nn.Embedding(vocab_size, embedding_size, padding_idx=0)\r\n",
    "        else:\r\n",
    "            # self.embedding = torch.nn.Embedding(vocab_size, embedding_size, padding_idx=vocab[\"<pad>\"])\r\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(vectors.vectors, freeze=True, padding_idx=vocab[\"<pad>\"])\r\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\r\n",
    "        self.linear = nn.Linear(lstm_hidden_size, num_class)\r\n",
    "        self.loss_function = nn.CrossEntropyLoss()\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.lstm_hidden_size = lstm_hidden_size\r\n",
    "    \r\n",
    "    def forward(self, X: torch.Tensor, lengths: torch.LongTensor):\r\n",
    "        x = self.embedding(X)\r\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths.to(\"cpu\"), enforce_sorted=False, batch_first=True)\r\n",
    "        _, (hn, _) = self.lstm(x)\r\n",
    "        # hn  = hn.view(hn.size(0), -1)\r\n",
    "        hn = hn[-1,:,:]\r\n",
    "        # hn = hn.transpose(0, 1) # batch_first\r\n",
    "        # hn = hn[:, -1:].flatten(1) # last layers only\r\n",
    "        # x = nn.functional.relu(hn)#.hnsqueeze(1)\r\n",
    "        x = self.linear(hn)\r\n",
    "        return x\r\n",
    "    \r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        x, y, lengths = batch\r\n",
    "        y_hat = self(x, lengths)\r\n",
    "        loss = self.loss_function(y_hat, y)\r\n",
    "        self.log(\"Train loss\", loss.detach())\r\n",
    "        return loss\r\n",
    "           \r\n",
    "    \r\n",
    "    def configure_optimizers(self):\r\n",
    "        return Adam(self.parameters(), lr=1e-2)\r\n",
    "    \r\n",
    "    # def train_dataloader(self):\r\n",
    "    #     return train_iter\r\n",
    "    \r\n",
    "    def test_step(self, batch, batch_idx):\r\n",
    "        x, y, lengths = batch\r\n",
    "        # x = pad_sequence(x, batch_first=True)\r\n",
    "        y_hat = self(x, lengths)\r\n",
    "        loss = self.loss_function(y_hat, y)\r\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\r\n",
    "        test_acc = torch.sum(labels_hat == y).item() / (len(y) * 1.0)\r\n",
    "        return self.log_dict({'test_loss': loss, 'test_acc': test_acc})\r\n",
    "    \r\n",
    "    def train_dataloader(self):\r\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn) # collate_fn=collate_fn\r\n",
    "\r\n",
    "    \r\n",
    "    def test_dataloader(self):\r\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn)\r\n",
    "    \r\n",
    "    # def test_epoch_end(self, outputs):\r\n",
    "    #     avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\r\n",
    "    #     tensorboard_logs = dict(\r\n",
    "    #         test_loss=avg_loss\r\n",
    "    #     )\r\n",
    "    #     return dict(\r\n",
    "    #         avg_test_loss=avg_loss, \r\n",
    "    #         log=tensorboard_logs\r\n",
    "    #     )\r\n",
    "    \r\n",
    "    # def test_dataloader(self):\r\n",
    "    #     return test_iter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab_size = train_dataset.vocab_size()\r\n",
    "model = LSTM(vocab_size, embedding_size=300, num_class=2, vocab=glove_vocab, vectors=glove_vec)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# xlist = []\r\n",
    "# ylist = []\r\n",
    "# for i in range(5):\r\n",
    "#     xlist.append(train_dataset[i][0])\r\n",
    "#     ylist.append(train_dataset[i][1])\r\n",
    "# with torch.no_grad():\r\n",
    "#     model.training_step([xlist, torch.tensor(ylist, dtype=torch.long)], 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pad_sequence(xlist, batch_first=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# xlist[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model.to('cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "zlist = []\r\n",
    "ylist = []\r\n",
    "for i in range(5):\r\n",
    "    zlist.append(train_dataset[i][0].clone())\r\n",
    "    ylist.append(train_dataset[i][1].clone())\r\n",
    "z0 = pad_sequence(zlist, batch_first=True)\r\n",
    "y = torch.tensor(ylist, dtype=torch.long)\r\n",
    "lengths = torch.LongTensor(list(map(len, zlist)))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for i in range(10007):\r\n",
    "#     model.embedding(torch.LongTensor([1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model(z0, lengths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# z1 = model.embedding(torch.tensor(z0))\r\n",
    "# z1.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# _, (z2, _) = model.lstm(z1)\r\n",
    "# z2.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# z3 = z2[0]\r\n",
    "# z3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# z4 = z3#.transpose(0, 1)\r\n",
    "# z4.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# z5 = z4#[:, -1:].flatten(1)\r\n",
    "# z5.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# z6 = model.linear(z5)\r\n",
    "# z6"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# loss = nn.functional.cross_entropy(z6, y)\r\n",
    "# loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def get_batch(idx, batch_size=10):\r\n",
    "#     y = [train_dataset[idx*batch_size+i][1] for i in range(batch_size//2)]\r\n",
    "#     x = [train_dataset[idx*batch_size+i][0] for i in range(batch_size//2)]\r\n",
    "\r\n",
    "#     y += [train_dataset[12500+idx*batch_size+i][1] for i in range(batch_size//2)]\r\n",
    "#     x += [train_dataset[12500+idx*batch_size+i][0] for i in range(batch_size//2)]\r\n",
    "#     return pad_sequence(x, batch_first=True), torch.tensor(y, dtype=torch.long)\r\n",
    "\r\n",
    "# i=0\r\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\r\n",
    "# epochs = 100\r\n",
    "# batch_size = 10\r\n",
    "# for e in range(epochs):\r\n",
    "#     mean_loss = 0\r\n",
    "#     for i in range(10):\r\n",
    "#         if i%2:\r\n",
    "#             batch = get_batch(i)\r\n",
    "#         else:\r\n",
    "#             batch = get_batch(i)\r\n",
    "#         x, y = batch\r\n",
    "#         y_hat = model(x)\r\n",
    "#         optimizer.zero_grad()\r\n",
    "#         loss = model.loss_function(y_hat, y)\r\n",
    "#         loss.backward()\r\n",
    "#         optimizer.step()\r\n",
    "#         mean_loss += loss.detach()\r\n",
    "#     print(mean_loss/10)\r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "    # loss = model.loss_function(y_hat, y)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\r\n",
    "logger = TensorBoardLogger('tb_logs', name='lstm')\r\n",
    "# tensorboard --logdir logs/1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.cuda.is_available()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer = Trainer(max_epochs=10, gpus=1, auto_select_gpus=True, auto_scale_batch_size=False, auto_lr_find=True, logger=[logger], track_grad_norm=2, \r\n",
    "accumulate_grad_batches=8)\r\n",
    "# gpus=1, auto_select_gpus=True\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# trainer.tune(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.learning_rate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "trainer.test(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.fit(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.test(model, test_dataloader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for item in test_dataset:\r\n",
    "    if item[0].max() > 10000:\r\n",
    "        print(f\"error: index {item[0].max()}\")\r\n",
    "    if item[0].min() < 0:\r\n",
    "        print(f\"error: index {item[0].min()}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for item in test_dataset:\r\n",
    "#     try:\r\n",
    "#         model(item[0].unsqueeze(0), lengths = torch.LongTensor(list(map(len, [item[0]]))))\r\n",
    "#     except:\r\n",
    "#         print(item[0])\r\n",
    "#         wrong_item = item[0].clone()\r\n",
    "#         break\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model.embedding(torch.LongTensor([10000]))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('deep_learning': conda)"
  },
  "interpreter": {
   "hash": "cfa97515b48ec924051b6584906c775d898061829fcbf4ab570c29d743c4fef7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}