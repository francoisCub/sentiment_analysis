{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook contains experimental training of LSTM with pytorch lightning\r\n",
    "\r\n",
    "There is still a bug when shuffling data for training (see dataloader) but it seems to work better without shuffling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pytorch_lightning\r\n",
    "from pytorch_lightning import Trainer, LightningModule\r\n",
    "import torch\r\n",
    "from data_classes.IMDB import IMDBClass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = IMDBClass()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\r\n",
    "# For use in DataLoader\r\n",
    "def collate_fn(batch):\r\n",
    "    x = [item[0].squeeze() for item in batch]\r\n",
    "    y = torch.tensor([item[1] for item in batch], dtype=torch.long)\r\n",
    "    return x, y\r\n",
    "\r\n",
    "# Warning: there is a bug somewhere in the code, when shuffle=True the model doesn't learn anymore\r\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=False, collate_fn=collate_fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.optim import Adam\r\n",
    "from torch import nn\r\n",
    "from torch.nn.utils.rnn import pad_sequence\r\n",
    "\r\n",
    "\r\n",
    "class LSTM(LightningModule):\r\n",
    "    def __init__(self, vocab_size, embedding_size=64, lstm_hidden_size=100, num_class=2):\r\n",
    "        super().__init__()\r\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\r\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\r\n",
    "        self.linear = nn.Linear(lstm_hidden_size, num_class)\r\n",
    "        self.loss_function = nn.CrossEntropyLoss()\r\n",
    "        self.batch_size = 10\r\n",
    "        self.learning_rate = 0.01\r\n",
    "    \r\n",
    "    def forward(self, X: torch.Tensor):\r\n",
    "        x = self.embedding(X)\r\n",
    "        _, (hn, cn) = self.lstm(x)\r\n",
    "        # hn  = hn.view(hn.size(0), -1)\r\n",
    "        x = nn.functional.relu(hn[0])#.hnsqueeze(1)\r\n",
    "        x = self.linear(x)\r\n",
    "        return x\r\n",
    "    \r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        x, y = batch\r\n",
    "        x = pad_sequence(x, batch_first=True)\r\n",
    "        y_hat = self(x)\r\n",
    "        loss = self.loss_function(y_hat, y)\r\n",
    "        self.log(\"Train loss\", loss.detach())\r\n",
    "        return loss\r\n",
    "           \r\n",
    "    \r\n",
    "    def configure_optimizers(self):\r\n",
    "        return Adam(self.parameters(), lr=1e-2)\r\n",
    "    \r\n",
    "    # def train_dataloader(self):\r\n",
    "    #     return train_iter\r\n",
    "    \r\n",
    "    def test_step(self, batch, batch_idx):\r\n",
    "        x, y = batch\r\n",
    "        y_hat = self(x)\r\n",
    "        loss = self.loss_function(y_hat, y)\r\n",
    "        return dict(\r\n",
    "            test_loss=loss,\r\n",
    "            log=dict(\r\n",
    "                test_loss=loss\r\n",
    "            )\r\n",
    "        )\r\n",
    "    \r\n",
    "    def test_epoch_end(self, outputs):\r\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\r\n",
    "        tensorboard_logs = dict(\r\n",
    "            test_loss=avg_loss\r\n",
    "        )\r\n",
    "        return dict(\r\n",
    "            avg_test_loss=avg_loss, \r\n",
    "            log=tensorboard_logs\r\n",
    "        )\r\n",
    "    \r\n",
    "    # def test_dataloader(self):\r\n",
    "    #     return test_iter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab_size = train_dataset.vocab_size()\r\n",
    "model = LSTM(vocab_size, num_class=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\r\n",
    "logger = TensorBoardLogger('tb_logs', name='lstm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer = Trainer(max_epochs=1, gpus=1, auto_select_gpus=True, auto_scale_batch_size=False, auto_lr_find=False, logger=[logger], track_grad_norm=2)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.tune(model, train_dataloader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.learning_rate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# trainer.test(model, test_dataloader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.fit(model, train_dataloader)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('deep_learning': conda)"
  },
  "interpreter": {
   "hash": "cfa97515b48ec924051b6584906c775d898061829fcbf4ab570c29d743c4fef7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}