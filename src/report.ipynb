{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web and Text Analytics - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "- Francois Cubelier\n",
    "- Lisa Bueres\n",
    "- Romain Charles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This report showcases our results for sentiment analysis. We experimented different types of recurrent neural networks on this task: basic RNN, LSTM and GRU. We studied the effect of attention on these models as well as testing different word embeddings (GloVe, FastText and Word2Vec). Finally, document level embedding were also investigated (WME, Bert sentence, GloVe average and a task specific average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Sentiment analysis is... TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### RNNs\n",
    "\n",
    "3 types of recurrent neural networks were tested: Base RNN, LSTM and GRUs. We also experimented with different hyperparameters: \n",
    "- The number of hidden layers: 1 or 2\n",
    "- The size of the hidden state: 100 or 300\n",
    "- The type of output layer after the main netwrok: linear or MLP (for more details on the MLP, see models/RNN.py)\n",
    "\n",
    "### Attention\n",
    "We tested 2 types of attention based on scaled dot-product attention: \n",
    "- Last hidden layer attention: \n",
    "$Attention(Q, K, V) = softmax(QK^\\top/\\sqrt n)V$ where V = K = output sequence and Q is the last hiddent state\n",
    "- Self-attention followed by last hiddent layer attention.\n",
    "First we apply a self attention:\n",
    "$Attention_1(Q, K, V) = V' = softmax(QK^\\top/\\sqrt n)V$ where V = K = Q = output sequence.\n",
    "Then we apply the last hidden layer attention:\n",
    "$Attention_2(Q', K', V') = softmax(Q'K'^\\top/\\sqrt n)V'$ where V' = K' = output sequence of self-attention and Q is the last hidden state of the RNN.\n",
    "\n",
    "\n",
    "### Document level embeddings\n",
    "There numerous document level embeddings in the literature.\n",
    "In tested 4 methods:\n",
    "- Average glove embeddings: a sentence is encoded by the average of its GloVe word embeddings.\n",
    "- Task specific average: a sentece is encoded by the average of its word embeddings and the word embedding are trained from scratch for this task.\n",
    "- WME: Document embedding based on the distance (based on Word Mover Distance) between the document and randomly generated documents.\n",
    "- Sentence BERT: a pre-trained document level embedding based on BERT (all-MiniLM-L6-v2 model).\n",
    "\n",
    "In order to predict the class from the document embedding, 2 types of model were tested: linear model and MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Hyperparameters\n",
    "In this experiment, we compare RNN, LSTM and GRU over the IMDB datasets and examine the results by varying hyperparameters.\n",
    "\n",
    "List hparams and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"results/exp1_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.groupby(\"Model Type\").mean()[[\"Test Acc\", \"Test Loss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.groupby(\"Output Layer Type\").mean()[[\"Test Acc\", \"Test Loss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.groupby(\"Number of Layers\").mean()[[\"Test Acc\", \"Test Loss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.groupby(\"Hidden Size\").mean()[[\"Test Acc\", \"Test Loss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.groupby([\"Model Type\",\"Output Layer Type\", \"Hidden Size\", \"Number of Layers\"], sort=True).mean()[[\"Test Acc\", \"Test Loss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"step\", y=\"Val Loss\", hue=\"Model Type\", data=df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Model Type\", y=\"Val Loss\", data=df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Val Acc\", y=\"Val Loss\", hue=\"Model Type\", data=df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"Test Acc\", y=\"Test Loss\", hue=\"Model Type\", data=df_1, kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Attention\n",
    "In this experiment, we analyse the effect of adding an attention layer for the sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Word level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Document level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Performance on different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "- Nils Reimers and Iryna Gurevych (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. CoRR, abs/1908.10084.\n",
    "- Lingfei Wu and Ian En-Hsu Yen and Kun Xu and Fangli Xu and Avinash Balakrishnan and Pin-Yu Chen and Pradeep Ravikumar and Michael J. Witbrock (2018). Word Mover's Embedding: From Word2Vec to Document Embedding. CoRR, abs/1811.01713.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c378ef774c888df488ef1362b4619fdc0da707343ceebda9eeba1b66c76a885e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('nlp3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
